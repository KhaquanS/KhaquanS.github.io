<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-09-08">
<meta name="description" content="We will be coding a basic Sparse Autoencoder (SAE) in pytorch and training it on the MNIST dataset.">

<title>Implementing Sparse Autoencoders from Scratch – Muhammad Khaquan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../files/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Muhammad Khaquan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../files/resume.pdf"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/muhammad-khaquan-8601b6226/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/KhaquanS"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Implementing Sparse Autoencoders from Scratch</h1>
                  <div>
        <div class="description">
          We will be coding a basic Sparse Autoencoder (SAE) in pytorch and training it on the MNIST dataset.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 8, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="need-for-unsupervised-techniques" class="level2">
<h2 class="anchored" data-anchor-id="need-for-unsupervised-techniques">Need for Unsupervised Techniques</h2>
<p>As you might have heard, the power of deep-learning often times is not merely in the results that the deep models are capable of generating but largely in the representations that they learn.</p>
<p>Before the deep learning revolution (that most people believe occurred with <span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span>), experts would hand engineer features for domain specific tasks. These features could be various edge detection filters for vision tasks (Harris Corner Detector filter detects corners by gauging the change in pixel intensities) among others. However, as the complexity of the downstream tasks that we wanted to accomplish increased, this particular method for learning features did not scale so well.</p>
<p>Then, the advent of unsupervised techniques for learning these features rather than hand crafting came to the fore. This shift in how we learn the features for some input distribution is one of the major breakthroughs in modern deep-learning. One particular class of such techniques is the autoencoder architecture.</p>
</section>
<section id="the-autoencoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-autoencoder-architecture">The Autoencoder Architecture</h2>
<div style="text-align: center;">
<p><img src="encoder.png" alt="Autoencoder Architecture" style="width:80%;height:80%"></p>
</div>
<p>The autoencoder architecture does not require any carefully labelled data to learn representations for the input. In fact, all you really need is the input data itself.</p>
<p>More conceretely, given an input <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>, the <strong>encoder</strong> part of the autoencoder maps the input <span class="math inline">\(\mathbf{x}\)</span> to a latent representation <span class="math inline">\(\mathbf{h} \in \mathbb{R}^m\)</span>. Consequently, the <strong>decoder</strong> part of the autoencoder then uses this latent represenation <span class="math inline">\(\mathbf{h}\)</span> to reconstruct the input <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>We can then tune the weights of both the whole autoencoder via <strong>Gradient Descent</strong> by seeing how far off the reconstructed version of the input is from the actual input. For gauging how far off the reconstruction is from the input, we can employ common loss functions such as the <strong>mean squared error</strong>. The better our reconstruction, the more potent our latent representation <span class="math inline">\(\mathbf{h}\)</span> (or features) become.</p>
<p>If you notice, the input <span class="math inline">\(\mathbf{x}\)</span> was part of <span class="math inline">\(\mathbb{R}^n\)</span> but the latent representation <span class="math inline">\(\mathbf{h}\)</span> is part of <span class="math inline">\(\mathbb{R}^m\)</span>. We can then have two distinct cases:</p>
<ol type="1">
<li><p><strong>Over-complete case</strong>: This is when <span class="math inline">\(\mathbf{m} \ge \mathbf{n}\)</span>. This case is prone to a major issue that we peek into in the subsequent section.</p></li>
<li><p><strong>Under-complete case</strong>: This is when <span class="math inline">\(\mathbf{m} \lt \mathbf{n}\)</span>.</p></li>
</ol>
</section>
<section id="need-for-regularisation-in-autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="need-for-regularisation-in-autoencoders">Need for Regularisation in Autoencoders</h2>
<p>As foreshadowed in the previous section, the over-complete autoencoders have greater capacity (more neurons) in the latent representation layer <span class="math inline">\(\mathbf{h}\)</span> than the number of features in the input <span class="math inline">\(\mathbf{x}\)</span>. This can lead to one major issue:</p>
<p>The nature of the objective in the autoencoder setting (low error reconstruction of the input data) means that the autoencoder with a large latent space capacity can naively learn to ‘copy’ the input features into the latent space instead of learning new, useful features and still display an incredibly low loss. This is a severe case of overfitting where our network memorises the input data and will suffer to generalise outside of the training domain.</p>
<p>Although this case ususally arises in the over-complete case, it is also possible in the under-complete case given the distribution of the input data is rather simple.</p>
<p>But the question then is: <strong>How do we avoid this catastrophic over-fitting?</strong>. A simple answer would be <strong>Regularisation</strong>.</p>
<p>More specifically, we will be looking into the sparsity contraint which is a form of regularisation that gives rise to the Sparse Autoencoder (SAE).</p>
</section>
<section id="the-sparse-autoencoder-sae" class="level1">
<h1>The Sparse Autoencoder (SAE)</h1>
<div style="text-align: center;">
<p><img src="sae.png" alt="Autoencoder Architecture" style="width:80%;height:80%"></p>
</div>
<p>The main idea behind the SAE is that instead of allowing all the <span class="math inline">\(\mathbf{m}\)</span> neurons in the latent space <span class="math inline">\(\mathbf{h}\)</span> fire completely, we would like these neurons on average to be activated only upto a level <span class="math inline">\(\rho\)</span> for a given batch of inputs.</p>
<p>In the subsequent sections, we will be doing the following:</p>
<ol type="1">
<li><p><strong>Build an SAE step-by-step from scratch in Pytorch.</strong></p></li>
<li><p><strong>Train our SAE on the MNIST dataset.</strong></p></li>
<li><p><strong>See how well our SAE is able to reconstruct the MNIST dataset (i.e., numbers from 0-9).</strong></p></li>
</ol>
<section id="importing-the-data" class="level3">
<h3 class="anchored" data-anchor-id="importing-the-data">Importing the Data</h3>
<div id="cell-6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> argparse</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seed(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.1307</span>,), (<span class="fl">0.3081</span>,))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.MNIST(</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">'./data'</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>train_dataset,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span> BATCH_SIZE,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> datasets.MNIST(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">'./data'</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,  <span class="co"># Use the test set as validation set</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>val_dataset,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 9912422/9912422 [06:20&lt;00:00, 26040.58it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 28881/28881 [00:00&lt;00:00, 31908.37it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 1648877/1648877 [01:04&lt;00:00, 25719.19it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 4542/4542 [00:00&lt;00:00, 15999.10it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
</section>
<section id="building-the-encoder" class="level3">
<h3 class="anchored" data-anchor-id="building-the-encoder">Building the Encoder</h3>
<p>Once we recieve the input data of shape <span class="math inline">\((batch\_size, 1, 28, 28)\)</span>, we will first have to flatten it into a vector of shape <span class="math inline">\((batch\_size, 1* 28 *28) = (batch\_size, 784)\)</span>.</p>
<p>Then, this flattened vector will pass through the encoder network. The encoder network in our case is just a simple feed-forward network with ReLU activations sandwiched between linear layers. The final output vector from the encoder network will be our latent representation <span class="math inline">\(\mathbf{h}\)</span> for the input.</p>
<p>A simple, single layered under-complete encoder would look like this:</p>
<div id="cell-11" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>images, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'images shape: </span><span class="sc">{</span>images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>flat_images <span class="op">=</span> images.view(images.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># Convert the images tensor into a vector</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Flattened images vector shape: </span><span class="sc">{</span>flat_images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>images shape: torch.Size([64, 1, 28, 28])
Flattened images vector shape: torch.Size([64, 784])</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A simple single layered encoder. The size of h = 20 (i.e., 20 neurons in the latent space so its an undercomplete autoencoder)</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">20</span>),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    nn.ReLU()</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> encoder(flat_images)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Shape of encoded images: </span><span class="sc">{</span>encoded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of encoded images: torch.Size([64, 20])</code></pre>
</div>
</div>
<p>On the other hand, a more complex under-complete encoder could have mutliple linear layers with non-linearities in between:</p>
<div id="cell-14" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>complex_encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">256</span>),</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">128</span>, <span class="dv">20</span>),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> complex_encoder(flat_images)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Shape of encoded images: </span><span class="sc">{</span>encoded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of encoded images: torch.Size([64, 20])</code></pre>
</div>
</div>
<p>The latent representations for the first 5 of these 64 input images look something like this:</p>
<div id="cell-16" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting tensor to numpy array </span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>encoded_np <span class="op">=</span> encoded.detach().numpy()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshaping each vector to 2D image dimensions (e.g., 5x4) for visualization</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>num_images <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>image_height <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>image_width <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the first 5 latent representations</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_images):</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, num_images, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> encoded_np[i].reshape(image_height, image_width)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Image </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="understanding-the-sparsity-contraint" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-sparsity-contraint">Understanding the Sparsity Contraint</h3>
<p>Before we move on to the decoder, we need to get a grip over what exactly the sparsity constraint is.</p>
<p>As I mentioned earlier, we pick a hyper-parameter called <span class="math inline">\(\rho\)</span> which decides that for a given batch of input images, how activated on average should each of the <span class="math inline">\(\mathbf{m}\)</span> neurons in the latent representation <span class="math inline">\(\mathbf{h}\)</span> be.</p>
<p>Putting it mathematically, we have the following set of simple equations:</p>
<p><span class="math display">\[\hat{\rho}_i = \frac{1}{N} \sum_{i=1}^N h_i(x)\]</span></p>
<p>Here <span class="math inline">\(\hat{\rho}_i\)</span> represents the average activation of the <span class="math inline">\(i^{th}\)</span> latent space neuron over a batch of input data with <span class="math inline">\(N\)</span> elements.</p>
<p>Now, the sparsity contraint says that we need to push the value of <span class="math inline">\(\hat{\rho}_i\)</span> close to <span class="math inline">\(\rho\)</span> for all <span class="math inline">\(i \in \{1,2, \dots, m\}\)</span> in the latent space.</p>
<p>A simple way to measure how far off a particular distribution is from some reference distribution is to use the KL-divergence. In our case, we need to measure how far off <span class="math inline">\(\hat{\rho}\)</span> is from <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[\sum_{i=1}^m [\rho \log \frac{\rho}{\hat{\rho}_i} + (1-\rho) \log \frac{1-\rho}{1-\hat{\rho}_i}] \]</span></p>
<p>This can be represented as:</p>
<p><span class="math display">\[\sum_{i=1}^m \text{KL}(\rho \| \hat{\rho}_i)\]</span></p>
<p>Now let’s code up the sparsity penalty. I would like you to quickly take note of a these three implementation details:</p>
<ol type="1">
<li><p><strong>Shape of <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\hat{\rho}\)</span></strong>: Since we average across the batch, the batch dimension will simply collapse so the <span class="math inline">\(\hat{\rho}\)</span> vector of an encoded vector of shape <span class="math inline">\((batch\_size, 20)\)</span> would be of shape <span class="math inline">\((20,)\)</span>.</p></li>
<li><p><strong>Sparsity target:</strong> This is nothing but the <span class="math inline">\(\rho\)</span> value.</p></li>
<li><p><strong>Sparsity lambda:</strong> This is a multiplier that we use to scale up/down the sparsity penalty. Larger <strong>sparsity lambda</strong> values mean a larger penalty in case <span class="math inline">\(\hat{\rho}\)</span> deviates from <span class="math inline">\(\rho\)</span>.</p></li>
</ol>
<div id="cell-19" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>sparsity_target <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sparsity_lambda <span class="op">=</span> <span class="fl">2e-2</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>rho_hat <span class="op">=</span> encoded.mean(dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># Average activation of each neuron across the input batch </span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> torch.ones_like(rho_hat) <span class="op">*</span> sparsity_target <span class="co"># The rho vector we can to converge rho_hat towards</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding a small epsilon to rho_hat and rho to prevent log(0) errors</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>rho_hat <span class="op">=</span> rho_hat <span class="op">+</span> <span class="fl">1e-8</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> rho <span class="op">+</span> <span class="fl">1e-8</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Rho shape: </span><span class="sc">{</span>rho<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> || Rho hat shape: </span><span class="sc">{</span>rho_hat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>kl_div <span class="op">=</span> <span class="op">-</span> (rho <span class="op">*</span> torch.log(rho_hat)) <span class="op">-</span> ((<span class="dv">1</span><span class="op">-</span>rho) <span class="op">*</span> torch.log(<span class="dv">1</span><span class="op">-</span>rho_hat))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>penalty <span class="op">=</span> sparsity_lambda <span class="op">*</span> kl_div.<span class="bu">sum</span>()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Sparsity Penalty: </span><span class="sc">{</span>penalty<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rho shape: torch.Size([20]) || Rho hat shape: torch.Size([20])
Sparsity Penalty: 0.0159</code></pre>
</div>
</div>
</section>
<section id="building-the-decoder" class="level3">
<h3 class="anchored" data-anchor-id="building-the-decoder">Building the Decoder</h3>
<p>Now that we have built the <strong>Encoder</strong> and also hopefully understood the <strong>sparsity constraint</strong>, we can move on to the final piece of our architecture, the <strong>Decoder</strong>.</p>
<p>As discussed earlier, the <strong>Decoder</strong> takes the latent representation <span class="math inline">\(\mathbf{h}\)</span> as input and reconstructs the input data <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>For the sake of convenience, I will define the <strong>Decoder</strong> architecture simply as the inverse of the encoder architecture in case the encoder architecture has multiple layers. This means if the encoder had linear layers of shapes <span class="math inline">\([784, 256, 128, 20]\)</span> then the decoder would have layers with the shape <span class="math inline">\([20, 128, 256, 784]\)</span>.</p>
<div id="cell-21" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>complex_decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">20</span>, <span class="dv">128</span>),</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">128</span>, <span class="dv">256</span>),</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">256</span>, <span class="dv">784</span>),</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> complex_decoder(encoded)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Deocded shape: </span><span class="sc">{</span>decoded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Deocded shape: torch.Size([64, 784])</code></pre>
</div>
</div>
<p>Now since the encoder and the decoder have completely random weights at this stage, we should expect the reconstructed images from the autoencoder to be completely noisy (should look something like static noise).</p>
<p>But, in the final section when we actually train the model, you will see how beautifully the latent space captures the features of the input distribution and hence leads to some promising reconstructions!</p>
<div id="cell-23" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting tensor to numpy array </span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>untrained_images <span class="op">=</span> decoded.view(<span class="dv">64</span>, <span class="dv">28</span>, <span class="dv">28</span>).detach().numpy()</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Shape of images: </span><span class="sc">{</span>untrained_images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>num_images <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>image_height <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>image_width <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the first 5 latent representations</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_images):</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, num_images, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> untrained_images[i].reshape(image_height, image_width)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Image </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of images: (64, 28, 28)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="reconstruction-error" class="level3">
<h3 class="anchored" data-anchor-id="reconstruction-error">Reconstruction Error</h3>
<p>Ok, before we move on to combining the bits and pieces into a full-fledged SAE architecture, we need to understand the reconstruction error. We essentially need to measure the following:</p>
<p><strong>How far off is my reconstructed image from the actual input image?</strong></p>
<p>In our case, we can measure this using the Mean Squared Error (MSE). The final loss of our SAE model will be a sum of MSE and the Sparsity Penalty.</p>
<div id="cell-25" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>MSE_loss <span class="op">=</span> F.mse_loss(decoded, flat_images)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'MSE Loss: </span><span class="sc">{</span>MSE_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE Loss: 0.9892</code></pre>
</div>
</div>
</section>
<section id="putting-everything-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-everything-together">Putting Everything Together</h3>
<p>In this section, we will simply stitch together the bits and pieces from all the sections and finally train our SAE on MNIST. Let’s see how far our training takes us from our initial noisy reconstructions …</p>
<div id="cell-27" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SAE(nn.Module):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dims, h_dims, sparsity_target, sparsity_lambda):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_dims <span class="op">=</span> in_dims</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_dims <span class="op">=</span> h_dims</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sparsity_target <span class="op">=</span> sparsity_target</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sparsity_lambda <span class="op">=</span> sparsity_lambda</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Encoder maps the input to a high/low dimensional latent space depending on the kind of AE being used.</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">        In the over-complete case, latent space is higher dimensional than input space while in the under-complete</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co">        case, the opposite is true. The sparsity in the SAE generally allows the AE to learn a useful latent representation </span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co">        even in the over-complete case by ensuring that the AE does not learn to naively 'copy' the input into the </span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co">        higher dimensional latent space. We will be using the ReLU non-linearity between the encoder layers. </span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_layers <span class="op">=</span> [nn.Linear(<span class="va">self</span>.in_dims, <span class="va">self</span>.h_dims[<span class="dv">0</span>]), nn.ReLU()]</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(<span class="va">self</span>.h_dims)):</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.enc_layers.append(nn.Linear(<span class="va">self</span>.h_dims[i <span class="op">-</span> <span class="dv">1</span>], <span class="va">self</span>.h_dims[i]))</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.enc_layers.append(nn.ReLU())</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="co">        The decoder on the other hand uses the latent space representation of the input and tries to reconstruct</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="co">        the input. In this case, I will assume the decoder follows a reverse structure of layers compared to the </span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder. </span></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.h_dims) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dec_layers <span class="op">=</span> [nn.Linear(<span class="va">self</span>.h_dims[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.h_dims[<span class="op">-</span><span class="dv">2</span>]), nn.ReLU()]</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reverse the order of hidden layers for decoding</span></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.h_dims) <span class="op">-</span> <span class="dv">2</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.dec_layers.append(nn.Linear(<span class="va">self</span>.h_dims[i], <span class="va">self</span>.h_dims[i <span class="op">-</span> <span class="dv">1</span>]))</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.dec_layers.append(nn.ReLU())</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final layer to reconstruct the input dimensions</span></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dec_layers.append(nn.Linear(<span class="va">self</span>.h_dims[<span class="dv">0</span>], <span class="va">self</span>.in_dims))</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dec_layers <span class="op">=</span> [nn.Linear(<span class="va">self</span>.h_dims[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.in_dims)]          </span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(<span class="op">*</span><span class="va">self</span>.enc_layers)</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(<span class="op">*</span><span class="va">self</span>.dec_layers)</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten x into a vector </span></span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># (batch_size, in_dim)</span></span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        encoded <span class="op">=</span> <span class="va">self</span>.encoder(x) <span class="co"># Push the input into the latent space (i.e., encode it)        </span></span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        decoded <span class="op">=</span> <span class="va">self</span>.decoder(encoded) <span class="co"># Reconstruct the input back from the latent space (i.e., decode it)</span></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> encoded, decoded</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> penalty(<span class="va">self</span>, encoded):</span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a><span class="co">        This is the sparsity penalty. This adds a contraint on the activation values of the neurons in </span></span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a><span class="co">        the latent space by forcing them to be on average centered around some target sparsity level.</span></span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a><span class="co">        This contraint prevents the latent space from simply being a naive copy of the input especially </span></span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a><span class="co">        in the over-complete case. </span></span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Generally, the KL divergence between the target sparsity vector and average latent space activation </span></span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a><span class="co">        vector is used to implement the penalty. </span></span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>        data_rho <span class="op">=</span> encoded.mean(dim<span class="op">=</span><span class="dv">0</span>) <span class="op">+</span> <span class="fl">1e-8</span> <span class="co"># Average activation of each neuron across the input batch </span></span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a>        rho <span class="op">=</span> (torch.ones_like(data_rho) <span class="op">*</span> <span class="va">self</span>.sparsity_target) <span class="op">+</span> <span class="fl">1e-8</span></span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a>        kl_div <span class="op">=</span> <span class="op">-</span> (rho <span class="op">*</span> torch.log(data_rho)) <span class="op">-</span> ((<span class="dv">1</span><span class="op">-</span>rho) <span class="op">*</span> torch.log(<span class="dv">1</span><span class="op">-</span>data_rho))</span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a>        penalty <span class="op">=</span> <span class="va">self</span>.sparsity_lambda <span class="op">*</span> kl_div.<span class="bu">sum</span>()</span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> penalty</span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, decoded, encoded):</span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a>        mse_loss <span class="op">=</span> F.mse_loss(decoded, x)</span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a>        sparsity_penalty <span class="op">=</span> <span class="va">self</span>.penalty(encoded)</span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mse_loss <span class="op">+</span> sparsity_penalty</span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x):</span>
<span id="cb29-81"><a href="#cb29-81" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-82"><a href="#cb29-82" aria-hidden="true" tabindex="-1"></a>        torch.no_grad()</span>
<span id="cb29-83"><a href="#cb29-83" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Flatten the input to (batch_size, in_dim)</span></span>
<span id="cb29-84"><a href="#cb29-84" aria-hidden="true" tabindex="-1"></a>        encoded <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb29-85"><a href="#cb29-85" aria-hidden="true" tabindex="-1"></a>        decoded <span class="op">=</span> <span class="va">self</span>.decoder(encoded)</span>
<span id="cb29-86"><a href="#cb29-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-87"><a href="#cb29-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> decoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-28" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_dataloader, n_epochs, optimizer, device):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="op">=</span> []</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        total_train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training loop</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data, _ <span class="kw">in</span> tqdm(train_dataloader, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">"</span>):</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>            data <span class="op">=</span> data.to(device)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            encoded, decoded <span class="op">=</span> model(data)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> model.loss(data, decoded, encoded)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>            total_train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate average training loss</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        avg_train_loss <span class="op">=</span> total_train_loss <span class="op">/</span> <span class="bu">len</span>(train_dataloader)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        train_losses.append(avg_train_loss)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss"> -- Train Loss: </span><span class="sc">{</span>avg_train_loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'-'</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-29" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_images(model, dataloader, device, num_images, save_plot):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    image, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))[:num_images]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.to(device)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    reconstructed <span class="op">=</span> model.sample(image)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    reconstructed <span class="op">=</span> reconstructed.cpu().detach().numpy()</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.cpu().numpy()</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot original and reconstructed images side by side</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, num_images, figsize<span class="op">=</span>(<span class="dv">15</span> , <span class="dv">6</span>))</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_images):</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot original images</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[<span class="dv">0</span>, i]</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        ax.imshow(image[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Original'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot reconstructed images</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[<span class="dv">1</span>, i]</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        ax.imshow(reconstructed[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Reconstructed'</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save_plot:</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create 'images' directory if it does not exist</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">'./images'</span>):</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>            os.makedirs(<span class="st">'./images'</span>)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save the plot to 'images/plot.png'</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        plt.savefig(<span class="st">'images/plot.png'</span>)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Hopefully none of the things in the <strong>SAE class</strong> look unfamiliar since we have discussed each component of the class before. Now, let’s first initialise a SAE model, then train it and finally visualise the results.</p>
<div id="cell-31" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>set_seed()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SAE(<span class="dv">784</span>, [<span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">20</span>], <span class="fl">1e-3</span>, <span class="fl">3e-2</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">3e-4</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>train_losses  <span class="op">=</span> train_model(model, train_dataloader, epochs, optimizer, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/80: 100%|██████████| 938/938 [00:05&lt;00:00, 181.28it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 1/80 -- Train Loss: 0.6318
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/80: 100%|██████████| 938/938 [00:04&lt;00:00, 194.38it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 2/80 -- Train Loss: 0.4717
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/80: 100%|██████████| 938/938 [00:04&lt;00:00, 195.81it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 3/80 -- Train Loss: 0.4031
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 4/80: 100%|██████████| 938/938 [00:05&lt;00:00, 174.81it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 4/80 -- Train Loss: 0.3630
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 5/80: 100%|██████████| 938/938 [00:04&lt;00:00, 194.68it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 5/80 -- Train Loss: 0.3315
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 6/80: 100%|██████████| 938/938 [00:04&lt;00:00, 197.29it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 6/80 -- Train Loss: 0.3128
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 7/80: 100%|██████████| 938/938 [00:04&lt;00:00, 195.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 7/80 -- Train Loss: 0.2996
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 8/80: 100%|██████████| 938/938 [00:04&lt;00:00, 195.11it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 8/80 -- Train Loss: 0.2881
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 9/80: 100%|██████████| 938/938 [00:04&lt;00:00, 190.29it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 9/80 -- Train Loss: 0.2789
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 10/80: 100%|██████████| 938/938 [00:05&lt;00:00, 178.49it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 10/80 -- Train Loss: 0.2718
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 11/80: 100%|██████████| 938/938 [00:05&lt;00:00, 182.95it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 11/80 -- Train Loss: 0.2659
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 12/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.95it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 12/80 -- Train Loss: 0.2609
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 13/80: 100%|██████████| 938/938 [00:05&lt;00:00, 178.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 13/80 -- Train Loss: 0.2565
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 14/80: 100%|██████████| 938/938 [00:05&lt;00:00, 175.69it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 14/80 -- Train Loss: 0.2530
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 15/80: 100%|██████████| 938/938 [00:05&lt;00:00, 172.32it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 15/80 -- Train Loss: 0.2494
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 16/80: 100%|██████████| 938/938 [00:05&lt;00:00, 175.09it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 16/80 -- Train Loss: 0.2464
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 17/80: 100%|██████████| 938/938 [00:05&lt;00:00, 174.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 17/80 -- Train Loss: 0.2437
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 18/80: 100%|██████████| 938/938 [00:05&lt;00:00, 168.45it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 18/80 -- Train Loss: 0.2412
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 19/80: 100%|██████████| 938/938 [00:05&lt;00:00, 170.91it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 19/80 -- Train Loss: 0.2389
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 20/80: 100%|██████████| 938/938 [00:05&lt;00:00, 172.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 20/80 -- Train Loss: 0.2371
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 21/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.86it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 21/80 -- Train Loss: 0.2346
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 22/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.76it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 22/80 -- Train Loss: 0.2319
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 23/80: 100%|██████████| 938/938 [00:05&lt;00:00, 166.63it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 23/80 -- Train Loss: 0.2292
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 24/80: 100%|██████████| 938/938 [00:05&lt;00:00, 168.12it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 24/80 -- Train Loss: 0.2269
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 25/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.56it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 25/80 -- Train Loss: 0.2248
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 26/80: 100%|██████████| 938/938 [00:05&lt;00:00, 165.68it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 26/80 -- Train Loss: 0.2226
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 27/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.63it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 27/80 -- Train Loss: 0.2203
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 28/80: 100%|██████████| 938/938 [00:05&lt;00:00, 166.43it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 28/80 -- Train Loss: 0.2180
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 29/80: 100%|██████████| 938/938 [00:05&lt;00:00, 166.70it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 29/80 -- Train Loss: 0.2162
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 30/80: 100%|██████████| 938/938 [00:05&lt;00:00, 165.14it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 30/80 -- Train Loss: 0.2145
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 31/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.07it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 31/80 -- Train Loss: 0.2130
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 32/80: 100%|██████████| 938/938 [00:05&lt;00:00, 168.54it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 32/80 -- Train Loss: 0.2116
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 33/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.63it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 33/80 -- Train Loss: 0.2103
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 34/80: 100%|██████████| 938/938 [00:05&lt;00:00, 163.08it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 34/80 -- Train Loss: 0.2090
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 35/80: 100%|██████████| 938/938 [00:05&lt;00:00, 161.42it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 35/80 -- Train Loss: 0.2078
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 36/80: 100%|██████████| 938/938 [00:05&lt;00:00, 163.56it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 36/80 -- Train Loss: 0.2067
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 37/80: 100%|██████████| 938/938 [00:05&lt;00:00, 167.25it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 37/80 -- Train Loss: 0.2056
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 38/80: 100%|██████████| 938/938 [00:05&lt;00:00, 168.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 38/80 -- Train Loss: 0.2047
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 39/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.40it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 39/80 -- Train Loss: 0.2034
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 40/80: 100%|██████████| 938/938 [00:05&lt;00:00, 161.36it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 40/80 -- Train Loss: 0.2024
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 41/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.95it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 41/80 -- Train Loss: 0.2015
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 42/80: 100%|██████████| 938/938 [00:05&lt;00:00, 163.41it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 42/80 -- Train Loss: 0.2006
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 43/80: 100%|██████████| 938/938 [00:05&lt;00:00, 164.01it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 43/80 -- Train Loss: 0.1999
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 44/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.84it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 44/80 -- Train Loss: 0.1990
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 45/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.58it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 45/80 -- Train Loss: 0.1982
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 46/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.97it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 46/80 -- Train Loss: 0.1976
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 47/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.43it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 47/80 -- Train Loss: 0.1968
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 48/80: 100%|██████████| 938/938 [00:06&lt;00:00, 143.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 48/80 -- Train Loss: 0.1961
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 49/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.24it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 49/80 -- Train Loss: 0.1954
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 50/80: 100%|██████████| 938/938 [00:05&lt;00:00, 161.25it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 50/80 -- Train Loss: 0.1948
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 51/80: 100%|██████████| 938/938 [00:05&lt;00:00, 162.08it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 51/80 -- Train Loss: 0.1942
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 52/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.26it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 52/80 -- Train Loss: 0.1936
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 53/80: 100%|██████████| 938/938 [00:05&lt;00:00, 161.30it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 53/80 -- Train Loss: 0.1930
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 54/80: 100%|██████████| 938/938 [00:06&lt;00:00, 153.67it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 54/80 -- Train Loss: 0.1922
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 55/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.90it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 55/80 -- Train Loss: 0.1915
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 56/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 56/80 -- Train Loss: 0.1909
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 57/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.12it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 57/80 -- Train Loss: 0.1904
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 58/80: 100%|██████████| 938/938 [00:06&lt;00:00, 148.96it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 58/80 -- Train Loss: 0.1898
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 59/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 59/80 -- Train Loss: 0.1891
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 60/80: 100%|██████████| 938/938 [00:05&lt;00:00, 157.97it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 60/80 -- Train Loss: 0.1884
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 61/80: 100%|██████████| 938/938 [00:05&lt;00:00, 157.66it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 61/80 -- Train Loss: 0.1875
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 62/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.33it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 62/80 -- Train Loss: 0.1865
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 63/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.25it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 63/80 -- Train Loss: 0.1853
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 64/80: 100%|██████████| 938/938 [00:06&lt;00:00, 154.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 64/80 -- Train Loss: 0.1845
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 65/80: 100%|██████████| 938/938 [00:05&lt;00:00, 157.46it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 65/80 -- Train Loss: 0.1836
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 66/80: 100%|██████████| 938/938 [00:05&lt;00:00, 161.59it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 66/80 -- Train Loss: 0.1829
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 67/80: 100%|██████████| 938/938 [00:05&lt;00:00, 160.56it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 67/80 -- Train Loss: 0.1822
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 68/80: 100%|██████████| 938/938 [00:05&lt;00:00, 162.73it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 68/80 -- Train Loss: 0.1815
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 69/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.04it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 69/80 -- Train Loss: 0.1810
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 70/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.72it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 70/80 -- Train Loss: 0.1802
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 71/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.58it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 71/80 -- Train Loss: 0.1797
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 72/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 72/80 -- Train Loss: 0.1792
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 73/80: 100%|██████████| 938/938 [00:05&lt;00:00, 158.52it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 73/80 -- Train Loss: 0.1786
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 74/80: 100%|██████████| 938/938 [00:06&lt;00:00, 155.61it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 74/80 -- Train Loss: 0.1781
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 75/80: 100%|██████████| 938/938 [00:06&lt;00:00, 155.55it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 75/80 -- Train Loss: 0.1775
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 76/80: 100%|██████████| 938/938 [00:05&lt;00:00, 156.91it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 76/80 -- Train Loss: 0.1771
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 77/80: 100%|██████████| 938/938 [00:05&lt;00:00, 159.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 77/80 -- Train Loss: 0.1766
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 78/80: 100%|██████████| 938/938 [00:05&lt;00:00, 156.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 78/80 -- Train Loss: 0.1762
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 79/80: 100%|██████████| 938/938 [00:05&lt;00:00, 157.26it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 79/80 -- Train Loss: 0.1756
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 80/80: 100%|██████████| 938/938 [00:06&lt;00:00, 140.10it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 80/80 -- Train Loss: 0.1753
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb194-2"><a href="#cb194-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Loss'</span>)</span>
<span id="cb194-3"><a href="#cb194-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb194-4"><a href="#cb194-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean Squared Error'</span>)</span>
<span id="cb194-5"><a href="#cb194-5" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses, color<span class="op">=</span><span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb194-6"><a href="#cb194-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb194-7"><a href="#cb194-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Look how much better the image reconstructions have now become! Obviously, the longer we train for, the better the results get.</p>
<div id="cell-34" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's sample some images from the trained SAE and see far we have come!</span></span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>plot_images(model, train_dataloader, device, <span class="dv">5</span>, <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If you noticed, the SAE that we just trained was a under-complete case, lets now move into a very simple over complete case and see how it fares!</p>
<div id="cell-36" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb196"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>set_seed()</span>
<span id="cb196-2"><a href="#cb196-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-3"><a href="#cb196-3" aria-hidden="true" tabindex="-1"></a>overcomplete_model <span class="op">=</span> SAE(<span class="dv">784</span>, [<span class="dv">1024</span>], <span class="fl">1e-3</span>, <span class="fl">3e-2</span>)</span>
<span id="cb196-4"><a href="#cb196-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-5"><a href="#cb196-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb196-6"><a href="#cb196-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb196-7"><a href="#cb196-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(overcomplete_model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb196-8"><a href="#cb196-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-9"><a href="#cb196-9" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')</span></span>
<span id="cb196-10"><a href="#cb196-10" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb196-11"><a href="#cb196-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-12"><a href="#cb196-12" aria-hidden="true" tabindex="-1"></a>train_losses  <span class="op">=</span> train_model(overcomplete_model, train_dataloader, epochs, optimizer, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/80:   0%|          | 0/938 [00:00&lt;?, ?it/s]Epoch 1/80: 100%|██████████| 938/938 [00:08&lt;00:00, 115.04it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 1/80 -- Train Loss: 1.4328
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/80: 100%|██████████| 938/938 [00:07&lt;00:00, 119.59it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 2/80 -- Train Loss: 1.1536
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.76it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 3/80 -- Train Loss: 1.0151
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 4/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.89it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 4/80 -- Train Loss: 0.9386
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 5/80: 100%|██████████| 938/938 [00:07&lt;00:00, 117.38it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 5/80 -- Train Loss: 0.8905
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 6/80: 100%|██████████| 938/938 [00:08&lt;00:00, 116.39it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 6/80 -- Train Loss: 0.8526
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 7/80: 100%|██████████| 938/938 [00:08&lt;00:00, 113.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 7/80 -- Train Loss: 0.8288
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 8/80: 100%|██████████| 938/938 [00:08&lt;00:00, 105.02it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 8/80 -- Train Loss: 0.8013
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 9/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.01it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 9/80 -- Train Loss: 0.7795
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 10/80: 100%|██████████| 938/938 [00:08&lt;00:00, 112.55it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 10/80 -- Train Loss: 0.7603
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 11/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.33it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 11/80 -- Train Loss: 0.7524
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 12/80: 100%|██████████| 938/938 [00:08&lt;00:00, 112.71it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 12/80 -- Train Loss: 0.7317
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 13/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.93it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 13/80 -- Train Loss: 0.7284
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 14/80: 100%|██████████| 938/938 [00:09&lt;00:00, 100.70it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 14/80 -- Train Loss: 0.7195
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 15/80: 100%|██████████| 938/938 [00:09&lt;00:00, 101.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 15/80 -- Train Loss: 0.7130
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 16/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.45it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 16/80 -- Train Loss: 0.7054
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 17/80: 100%|██████████| 938/938 [00:08&lt;00:00, 105.15it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 17/80 -- Train Loss: 0.6976
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 18/80: 100%|██████████| 938/938 [00:08&lt;00:00, 112.27it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 18/80 -- Train Loss: 0.6920
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 19/80: 100%|██████████| 938/938 [00:08&lt;00:00, 113.31it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 19/80 -- Train Loss: 0.6832
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 20/80: 100%|██████████| 938/938 [00:08&lt;00:00, 112.05it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 20/80 -- Train Loss: 0.6778
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 21/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 21/80 -- Train Loss: 0.6838
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 22/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.65it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 22/80 -- Train Loss: 0.6745
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 23/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.62it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 23/80 -- Train Loss: 0.6696
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 24/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.22it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 24/80 -- Train Loss: 0.6674
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 25/80: 100%|██████████| 938/938 [00:08&lt;00:00, 110.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 25/80 -- Train Loss: 0.6696
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 26/80: 100%|██████████| 938/938 [00:09&lt;00:00, 96.43it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 26/80 -- Train Loss: 0.6655
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 27/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 27/80 -- Train Loss: 0.6611
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 28/80: 100%|██████████| 938/938 [00:08&lt;00:00, 112.20it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 28/80 -- Train Loss: 0.6561
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 29/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.98it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 29/80 -- Train Loss: 0.6583
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 30/80: 100%|██████████| 938/938 [00:08&lt;00:00, 104.31it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 30/80 -- Train Loss: 0.6574
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 31/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.87it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 31/80 -- Train Loss: 0.6519
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 32/80: 100%|██████████| 938/938 [00:09&lt;00:00, 102.45it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 32/80 -- Train Loss: 0.6536
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 33/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.91it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 33/80 -- Train Loss: 0.6525
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 34/80: 100%|██████████| 938/938 [00:08&lt;00:00, 110.99it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 34/80 -- Train Loss: 0.6516
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 35/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.65it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 35/80 -- Train Loss: 0.6485
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 36/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.99it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 36/80 -- Train Loss: 0.6440
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 37/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.30it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 37/80 -- Train Loss: 0.6410
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 38/80: 100%|██████████| 938/938 [00:08&lt;00:00, 111.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 38/80 -- Train Loss: 0.6393
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 39/80: 100%|██████████| 938/938 [00:08&lt;00:00, 104.94it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 39/80 -- Train Loss: 0.6405
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 40/80: 100%|██████████| 938/938 [00:09&lt;00:00, 98.89it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 40/80 -- Train Loss: 0.6436
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 41/80: 100%|██████████| 938/938 [00:09&lt;00:00, 101.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 41/80 -- Train Loss: 0.6365
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 42/80: 100%|██████████| 938/938 [00:08&lt;00:00, 105.37it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 42/80 -- Train Loss: 0.6371
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 43/80: 100%|██████████| 938/938 [00:08&lt;00:00, 107.56it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 43/80 -- Train Loss: 0.6364
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 44/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.42it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 44/80 -- Train Loss: 0.6325
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 45/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 45/80 -- Train Loss: 0.6341
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 46/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.24it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 46/80 -- Train Loss: 0.6349
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 47/80: 100%|██████████| 938/938 [00:08&lt;00:00, 105.68it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 47/80 -- Train Loss: 0.6374
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 48/80: 100%|██████████| 938/938 [00:09&lt;00:00, 103.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 48/80 -- Train Loss: 0.6339
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 49/80: 100%|██████████| 938/938 [00:09&lt;00:00, 101.69it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 49/80 -- Train Loss: 0.6311
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 50/80: 100%|██████████| 938/938 [00:09&lt;00:00, 102.08it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 50/80 -- Train Loss: 0.6356
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 51/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.20it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 51/80 -- Train Loss: 0.6330
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 52/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 52/80 -- Train Loss: 0.6280
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 53/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.70it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 53/80 -- Train Loss: 0.6293
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 54/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.17it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 54/80 -- Train Loss: 0.6304
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 55/80: 100%|██████████| 938/938 [00:08&lt;00:00, 110.15it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 55/80 -- Train Loss: 0.6278
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 56/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.93it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 56/80 -- Train Loss: 0.6340
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 57/80: 100%|██████████| 938/938 [00:09&lt;00:00, 99.92it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 57/80 -- Train Loss: 0.6375
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 58/80: 100%|██████████| 938/938 [00:09&lt;00:00, 103.86it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 58/80 -- Train Loss: 0.6328
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 59/80: 100%|██████████| 938/938 [00:09&lt;00:00, 102.29it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 59/80 -- Train Loss: 0.6290
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 60/80: 100%|██████████| 938/938 [00:09&lt;00:00, 101.81it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 60/80 -- Train Loss: 0.6256
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 61/80: 100%|██████████| 938/938 [00:09&lt;00:00, 99.03it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 61/80 -- Train Loss: 0.6256
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 62/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.02it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 62/80 -- Train Loss: 0.6230
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 63/80: 100%|██████████| 938/938 [00:09&lt;00:00, 101.12it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 63/80 -- Train Loss: 0.6251
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 64/80: 100%|██████████| 938/938 [00:09&lt;00:00, 95.23it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 64/80 -- Train Loss: 0.6246
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 65/80: 100%|██████████| 938/938 [00:11&lt;00:00, 83.55it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 65/80 -- Train Loss: 0.6267
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 66/80: 100%|██████████| 938/938 [00:08&lt;00:00, 104.23it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 66/80 -- Train Loss: 0.6200
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 67/80: 100%|██████████| 938/938 [00:08&lt;00:00, 109.93it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 67/80 -- Train Loss: 0.6293
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 68/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.93it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 68/80 -- Train Loss: 0.6249
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 69/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.84it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 69/80 -- Train Loss: 0.6249
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 70/80: 100%|██████████| 938/938 [00:08&lt;00:00, 107.53it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 70/80 -- Train Loss: 0.6214
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 71/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 71/80 -- Train Loss: 0.6217
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 72/80: 100%|██████████| 938/938 [00:09&lt;00:00, 103.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 72/80 -- Train Loss: 0.6181
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 73/80: 100%|██████████| 938/938 [00:08&lt;00:00, 104.41it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 73/80 -- Train Loss: 0.6224
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 74/80: 100%|██████████| 938/938 [00:08&lt;00:00, 108.77it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 74/80 -- Train Loss: 0.6266
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 75/80: 100%|██████████| 938/938 [00:09&lt;00:00, 103.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 75/80 -- Train Loss: 0.6254
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 76/80: 100%|██████████| 938/938 [00:08&lt;00:00, 110.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 76/80 -- Train Loss: 0.6186
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 77/80: 100%|██████████| 938/938 [00:09&lt;00:00, 103.54it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 77/80 -- Train Loss: 0.6248
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 78/80: 100%|██████████| 938/938 [00:08&lt;00:00, 104.45it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 78/80 -- Train Loss: 0.6217
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 79/80: 100%|██████████| 938/938 [00:08&lt;00:00, 106.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 79/80 -- Train Loss: 0.6250
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 80/80: 100%|██████████| 938/938 [00:08&lt;00:00, 105.42it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Epoch: 80/80 -- Train Loss: 0.6249
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb358"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb358-1"><a href="#cb358-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb358-2"><a href="#cb358-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Loss'</span>)</span>
<span id="cb358-3"><a href="#cb358-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb358-4"><a href="#cb358-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean Squared Error'</span>)</span>
<span id="cb358-5"><a href="#cb358-5" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses, color<span class="op">=</span><span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb358-6"><a href="#cb358-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb358-7"><a href="#cb358-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb359"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb359-1"><a href="#cb359-1" aria-hidden="true" tabindex="-1"></a>plot_images(overcomplete_model, train_dataloader, device, <span class="dv">5</span>, <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>So in this blog post, we first saw the need for <strong>Unsupervised Learning</strong> techniques in the domain of feature learning, then we looked at a basic <strong>Autoencoder</strong> architecture, from there we finally built out the idea of a regularised <strong>Sparse Autoencoder</strong> and discussed the sparsity constraint in detail.</p>
<p>I hope I was able to convery the need to regularise the vanilla autoencoder especially in the overcomplete case. Moreover, I hope the final generations for both the undercomplete and overcomplete networks provide a good idea of how powerful these autoencoders are.</p>
<p>I will now be focusing on <strong>Variational Autoencoders</strong> in my next blog post and hopefully, we will be able to discover the generative capabilities of the VAEs!</p>
<p><strong>PS: Before I sign off, you can access my repo for the Sparse Autoencoder <a href="https://github.com/KhaquanS/Sparse_Autoencoder">from here</a> . You can simply run the sae.py file in your terminal to train your own SAE!</strong></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>